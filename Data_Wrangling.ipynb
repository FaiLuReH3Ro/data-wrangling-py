{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12934488,"sourceType":"datasetVersion","datasetId":8184950}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Data Wrangling**","metadata":{}},{"cell_type":"markdown","source":"## Repository\n\nCheck out the repository on [GitHub](https://github.com/FaiLuReH3Ro/data-wrangling-py) for more details.\n","metadata":{"vscode":{"languageId":"plaintext"}}},{"cell_type":"markdown","source":"## Dataset Used\n\n[Stack Overflow Survey Data 2024 Subset](https://www.kaggle.com/datasets/failureh3ro/stack-overflow-survey-data-2024-subset/data)","metadata":{}},{"cell_type":"markdown","source":"## Objectives\n\n* Identify duplicate rows in the dataset\n* Remove duplicate rows and verify the removal \n* Find columns with missing values\n* Impute the missing values\n* Perform Data Normalizing for certain columns","metadata":{"vscode":{"languageId":"latex"}}},{"cell_type":"markdown","source":"## Download and Import Libraries","metadata":{}},{"cell_type":"code","source":"# Run this cell if the libraries are not installed yet\n# Uncomment the lines below to install\n# %pip install pandas\n# %pip install numpy","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing the pandas and numpy libraries\nimport pandas as pd\nimport numpy as np\n\n# Suppress warnings\n# Comment before running to view warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the Data","metadata":{}},{"cell_type":"code","source":"# Read the CSV file into a dataframe\nurl = '/kaggle/input/stack-overflow-survey-data-2024-subset/survey_data.csv'\ndata = pd.read_csv(url)\n\n# Options to display all rows and columns\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\n# Display the first five rows\ndata.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handling Duplicate Rows","metadata":{}},{"cell_type":"markdown","source":"### Finding Duplicates","metadata":{}},{"cell_type":"code","source":"# Finding the number of duplicate rows\ndup_rows = data[data.duplicated()]\nnum_dups = dup_rows.shape[0]\nprint(f'There are {num_dups} duplicate rows')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Removing Duplicates","metadata":{}},{"cell_type":"code","source":"# Removing the duplicate rows and verifying\ndf = data.drop_duplicates()\nnum_dups = df[df.duplicated()].shape[0]\nprint(f'There are {num_dups} duplicate rows')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handling Missing Values","metadata":{}},{"cell_type":"markdown","source":"### Finding Missing Values","metadata":{}},{"cell_type":"code","source":"# This method displays the non-null counts for each column\n# The total rows is 65437 \n# Every column with less than the total row count means it has missing values \ndf.info(verbose=True, show_counts=True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# This find the top 5 columns with the most missing values\ndf.isnull().sum().sort_values(ascending = False).head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Droping Rows Based on Objective\n\nOur original goal is to find data about the technologies. In that case, the most important columns are `LanguageHaveWorkedWith`, `LanguageWantToWorkWith`, `DatabaseHaveWorkedWith`,`DatabaseWantToWorkWith`, `PlatformHaveWorkedWith`, `PlatformWantToWorkWith`, `WebframeHaveWorkedWith`, `WebframeWantToWorkWith`, `ToolsTechHaveWorkedWith`, `ToolsTechWantToWorkWith`, `NEWCollabToolsHaveWorkedWith`, `NEWCollabToolsWantToWorkWith`. \n\nTo get the most accurate date, it is best not to replace the missing values with the most frequent because that would skew the data. Here, I will find how many missing values are in these columns.","metadata":{}},{"cell_type":"code","source":"target_columns = ['LanguageHaveWorkedWith', 'LanguageWantToWorkWith', 'DatabaseHaveWorkedWith', \n                  'DatabaseWantToWorkWith', 'PlatformHaveWorkedWith', 'PlatformWantToWorkWith', \n                  'WebframeHaveWorkedWith', 'WebframeWantToWorkWith', 'ToolsTechHaveWorkedWith',\n                  'ToolsTechWantToWorkWith', 'NEWCollabToolsHaveWorkedWith', 'NEWCollabToolsWantToWorkWith']\n\nfor column in target_columns:\n    print(df[column].isnull().value_counts())\n    print(\"\")","metadata":{"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To not remove an excessive amount of data, I will drop the rows with the least amount of NaN values. Based on that, it seems like the `LanguageHaveWorkedWith` column has the least.                          ","metadata":{}},{"cell_type":"code","source":"# This will drop rows based only on the LanguageHaveWorkedWith column\ndf.dropna(subset=['LanguageHaveWorkedWith'], inplace = True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the process\ndf['LanguageHaveWorkedWith'].isnull().value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dropping Columns Based on Objective\n\nThere are many columns that I don't need in order to answer my questions. Therefore, I should only include relevant columns. This will also make the dataset smaller as well and easier to load.","metadata":{}},{"cell_type":"code","source":"df = df[['ResponseId', 'MainBranch', 'Age', 'Employment', 'RemoteWork', 'EdLevel', 'YearsCode',\n         'YearsCodePro', 'DevType', 'Country', 'CompTotal', 'LanguageHaveWorkedWith', \n         'LanguageWantToWorkWith', 'DatabaseHaveWorkedWith', 'DatabaseWantToWorkWith', \n         'PlatformHaveWorkedWith', 'PlatformWantToWorkWith', 'WebframeHaveWorkedWith', \n         'WebframeWantToWorkWith', 'ToolsTechHaveWorkedWith', 'ToolsTechWantToWorkWith', \n         'NEWCollabToolsHaveWorkedWith', 'NEWCollabToolsWantToWorkWith', 'WorkExp', 'ConvertedCompYearly',\n         'JobSat']]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imputing Missing Values in Numeric Columns","metadata":{}},{"cell_type":"markdown","source":"> Note: Since there are over 50% of missing values in some of the columns, I will not impute values for those columns. I will just leave the NaN values there for now.","metadata":{}},{"cell_type":"markdown","source":"#### YearsCode Column\n\nSince the `YearsCode` column has a dtype of 'object', I will convert it to 'float' for imputing the average years. ","metadata":{}},{"cell_type":"code","source":"# Method 1 - fillna\n\n# Replacing the strings to a number\ndf['YearsCode'].replace('Less than 1 year', '0', inplace = True)\ndf['YearsCode'].replace('More than 50 years', '50', inplace = True)\ndf['YearsCode'] = df['YearsCode'].astype(float)\n\n# Replace NaN values with the average\navg_years = round(df['YearsCode'].mean(), 0)\ndf['YearsCode'].fillna(avg_years, inplace = True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the imputation\ndf['YearsCode'].isnull().value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### YearsCodePro Column\n\nThe same thing applies to the `YearsCodePro` column. I will also convert it to 'float'.","metadata":{}},{"cell_type":"code","source":"# Method 2 - replace\n\n# Replacing the strings to a number\ndf['YearsCodePro'].replace('Less than 1 year', '0', inplace = True)\ndf['YearsCodePro'].replace('More than 50 years', '50', inplace = True)\ndf['YearsCodePro'] = df['YearsCodePro'].astype(float)\n\n# Replace NaN values with the average\navg_years = round(df['YearsCodePro'].mean(), 0)\ndf['YearsCodePro'].fillna(avg_years, inplace = True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the imputation\ndf['YearsCodePro'].isnull().value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Imputing Missing Values in Categorical Columns","metadata":{}},{"cell_type":"markdown","source":"#### RemoteWork Column","metadata":{}},{"cell_type":"code","source":"# Method 1 - fillna\n\n# Finding the most frequent value\nmost_remote = df['RemoteWork'].mode()[0]\n\n# Filling the missing values\ndf['RemoteWork'].fillna(most_remote, inplace = True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the imputation\ndf['RemoteWork'].isnull().value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### EdLevel Column","metadata":{}},{"cell_type":"code","source":"# Method 2 - replace\n\n# Finding the most frequent value\nfreq_ed_level = df['EdLevel'].mode()[0]\n\n# Replace the NaN with most frequent value\ndf['EdLevel'].replace(np.nan, freq_ed_level, inplace = True)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify the imputation\ndf['EdLevel'].isnull().value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Normalization","metadata":{}},{"cell_type":"markdown","source":"### Min-Max Scaling\n\n(data - min) / (max - min)","metadata":{}},{"cell_type":"markdown","source":"#### YearsCode Column","metadata":{}},{"cell_type":"code","source":"# Creating a new column to place the normalize values\ndf['YearsCode_MinMax'] = (df['YearsCode'] - df['YearsCode'].min()) / (df['YearsCode'].max() - df['YearsCode'].min())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare the normalized and original values\ndf[['YearsCode_MinMax', 'YearsCode']].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Z-score normalization\n\n(data - mean) / standard deviation","metadata":{}},{"cell_type":"markdown","source":"#### YearsCodePro Column","metadata":{}},{"cell_type":"code","source":"# Placing the normalize values in a new column\ndf['YearsCodePro_Zscore'] = (df['YearsCodePro'] - df['YearsCodePro'].mean()) / df['YearsCodePro'].std()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compare the normalized and original values\ndf[['YearsCodePro_Zscore', 'YearsCodePro']].head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Other Techniques","metadata":{}},{"cell_type":"markdown","source":"### Binning\n\nCreating a new column: 'ExperienceLevel' based on the 'YearsCodePro' Column","metadata":{}},{"cell_type":"code","source":"# Create the ranges and labels\nranges = [0, 3, 5, 8, 10, 100]\n\n# Store the names for each range\nrange_labels = ['Entry', 'Mid', 'Senior', 'Lead', 'Architect']\n\n# Using the function cut to apply the bins\ndf['ExperienceLevel'] = pd.cut(df['YearsCodePro'], bins = ranges, labels = range_labels, include_lowest=True, ordered=False)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Displaying 10 rows\ndf[['YearsCodePro', 'ExperienceLevel']].sample(n = 10, random_state = 42)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### One-hot Encoding\n\n","metadata":{}},{"cell_type":"code","source":"# Display the values and counts in the MainBranch column\ndf['MainBranch'].value_counts()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Using the method get_dummies to encode the MainBranch column\n# Values only consist of True or False\n# Rename the columns for better readability\ndf_encoded = pd.get_dummies(df['MainBranch'])\ndf_encoded.columns = ['ProDeveloper', 'Learner', 'OccasionalCoder', 'HobbyCoder', 'FormerDev']\ndf_encoded.head()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adding the new encoded values to the dataframe\nnew_df = pd.concat([df, df_encoded], axis = 1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Exporting the Dataframe to a CSV ","metadata":{}},{"cell_type":"code","source":"df.to_csv(\"clean_survey_data.csv\", index = False)","metadata":{},"outputs":[],"execution_count":null}]}